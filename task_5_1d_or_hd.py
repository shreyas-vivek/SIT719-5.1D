# -*- coding: utf-8 -*-
"""Task 5.1D or HD

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dCW1uEWEKCPQiY5SCQyBpEt5G3exaWUD

# **Section 1: Declare the Modules**
"""

import os
from collections import defaultdict
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, precision_score, recall_score, classification_report
from sklearn.metrics import ConfusionMatrixDisplay
from sklearn.preprocessing import StandardScaler
import seaborn as sns
import time


import warnings
warnings.filterwarnings('ignore')

"""# **Section 2: Data Import and Preprocess**"""

!pip install wget
import wget

link_to_data = 'https://raw.githubusercontent.com/SIT719/2020-S2/master/data/Week_5_NSL-KDD-Dataset/training_attack_types.txt?raw=true'
DataSet = wget.download(link_to_data)

DataSet

header_names = ['duration', 'protocol_type', 'service', 'flag', 'src_bytes', 'dst_bytes', 'land', 'wrong_fragment', 'urgent', 'hot', 'num_failed_logins', 'logged_in', 'num_compromised', 'root_shell', 'su_attempted', 'num_root', 'num_file_creations', 'num_shells', 'num_access_files', 'num_outbound_cmds', 'is_host_login', 'is_guest_login', 'count', 'srv_count', 'serror_rate', 'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate', 'same_srv_rate', 'diff_srv_rate', 'srv_diff_host_rate', 'dst_host_count', 'dst_host_srv_count', 'dst_host_same_srv_rate', 'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate', 'dst_host_serror_rate', 'dst_host_srv_serror_rate', 'dst_host_rerror_rate', 'dst_host_srv_rerror_rate', 'attack_type', 'success_pred']


# Differentiating between nominal, binary, and numeric features

# root_shell is marked as a continuous feature in the kddcup.names
# file, but it is supposed to be a binary feature according to the
# dataset documentation

# training_attack_types.txt maps each of the 22 different attacks to 1 of 4 categories
# file obtained from http://kdd.ics.uci.edu/databases/kddcup99/training_attack_types

col_names = np.array(header_names)

nominal_idx = [1, 2, 3]
binary_idx = [6, 11, 13, 14, 20, 21]
numeric_idx = list(set(range(41)).difference(nominal_idx).difference(binary_idx))

nominal_cols = col_names[nominal_idx].tolist()
binary_cols = col_names[binary_idx].tolist()
numeric_cols = col_names[numeric_idx].tolist()

# training_attack_types.txt maps each of the 22 different attacks to 1 of 4 categories
# file obtained from http://kdd.ics.uci.edu/databases/kddcup99/training_attack_types

category = defaultdict(list)
category['benign'].append('normal')

with open(DataSet, 'r') as f:
    for line in f.readlines():
        attack, cat = line.strip().split(' ')
        category[cat].append(attack)

attack_mapping = dict((v,k) for k in category for v in category[k])

attack_mapping

#Processing Training Data

train_file='https://raw.githubusercontent.com/SIT719/2020-S2/master/data/Week_5_NSL-KDD-Dataset/KDDTrain%2B.txt'



train_df = pd.read_csv(train_file, names=header_names)

train_df['attack_category'] = train_df['attack_type'].map(lambda x: attack_mapping[x])

train_df.drop(['success_pred'], axis=1, inplace=True)

#Processing test Data
test_file='https://raw.githubusercontent.com/SIT719/2020-S2/master/data/Week_5_NSL-KDD-Dataset/KDDTest%2B.txt'

test_df = pd.read_csv(test_file, names=header_names)
test_df['attack_category'] = test_df['attack_type'].map(lambda x: attack_mapping[x])
test_df.drop(['success_pred'], axis=1, inplace=True)

train_attack_types = train_df['attack_type'].value_counts()
train_attack_cats = train_df['attack_category'].value_counts()

test_attack_types = test_df['attack_type'].value_counts()
test_attack_cats = test_df['attack_category'].value_counts()

train_attack_types.plot(kind='barh', figsize=(20,10), fontsize=20)

train_attack_cats.plot(kind='barh', figsize=(20,10), fontsize=30)

train_df[binary_cols].describe().transpose()
train_df.groupby(['su_attempted']).size()
train_df['su_attempted'].replace(2, 0, inplace=True)
test_df['su_attempted'].replace(2, 0, inplace=True)
train_df.groupby(['su_attempted']).size()
train_df.groupby(['num_outbound_cmds']).size()

#Now, that's not a very useful feature - let's drop it from the dataset

train_df.drop('num_outbound_cmds', axis = 1, inplace=True)
test_df.drop('num_outbound_cmds', axis = 1, inplace=True)
numeric_cols.remove('num_outbound_cmds')

#Data Preparation

train_Y = train_df['attack_category']
train_x_raw = train_df.drop(['attack_category','attack_type'], axis=1)
test_Y = test_df['attack_category']
test_x_raw = test_df.drop(['attack_category','attack_type'], axis=1)


combined_df_raw = pd.concat([train_x_raw, test_x_raw])
combined_df = pd.get_dummies(combined_df_raw, columns=nominal_cols, drop_first=True)

train_x = combined_df[:len(train_x_raw)]
test_x = combined_df[len(train_x_raw):]

# Store dummy variable feature names
dummy_variables = list(set(train_x)-set(combined_df_raw))

#execute the commands in console
train_x.describe()
train_x['duration'].describe()
# Experimenting with StandardScaler on the single 'duration' feature
from sklearn.preprocessing import StandardScaler

durations = train_x['duration'].values.reshape(-1, 1)
standard_scaler = StandardScaler().fit(durations)
scaled_durations = standard_scaler.transform(durations)
pd.Series(scaled_durations.flatten()).describe()

# Experimenting with MinMaxScaler on the single 'duration' feature
from sklearn.preprocessing import MinMaxScaler

min_max_scaler = MinMaxScaler().fit(durations)
min_max_scaled_durations = min_max_scaler.transform(durations)
pd.Series(min_max_scaled_durations.flatten()).describe()

# Experimenting with RobustScaler on the single 'duration' feature
from sklearn.preprocessing import RobustScaler

min_max_scaler = RobustScaler().fit(durations)
robust_scaled_durations = min_max_scaler.transform(durations)
pd.Series(robust_scaled_durations.flatten()).describe()

# Experimenting with MaxAbsScaler on the single 'duration' feature
from sklearn.preprocessing import MaxAbsScaler

max_Abs_scaler = MaxAbsScaler().fit(durations)
robust_scaled_durations = max_Abs_scaler.transform(durations)
pd.Series(robust_scaled_durations.flatten()).describe()

# Let's proceed with StandardScaler- Apply to all the numeric columns

standard_scaler = StandardScaler().fit(train_x[numeric_cols])

train_x[numeric_cols] = \
    standard_scaler.transform(train_x[numeric_cols])

test_x[numeric_cols] = \
    standard_scaler.transform(test_x[numeric_cols])

train_x.describe()



train_Y_bin = train_Y.apply(lambda x: 0 if x is 'benign' else 1)
test_Y_bin = test_Y.apply(lambda x: 0 if x is 'benign' else 1)

def evaluate_classifier(class_type, train_x, train_Y, test_x, test_Y, label_names=None, **kwargs):
    classifier = class_type(**kwargs)
    classifier.fit(train_x, train_Y)

   # Record start time for training
    start_train_time = time.time()
    classifier.fit(train_x, train_Y)
    end_train_time = time.time()
    train_time = end_train_time - start_train_time  # Calculate training time

    # Make predictions on the test data
    start_test_time = time.time()
    y_pred = classifier.predict(test_x)
    end_test_time = time.time()
    test_time = end_test_time - start_test_time  # Calculate test time

    y_pred = classifier.predict(test_x)
    class_name = classifier.__class__.__name__
    print(f"Classifier: {class_name}")
    print(f"Evaluation Report for {class_name}:")
    show_evaluation_results(test_Y, y_pred, label_names, train_time, test_time)
    print("=" * 40)

performance_metrics = []

def show_evaluation_results(test_Y, y_pred, label_names, train_time, test_time):
    accuracy = accuracy_score(test_Y, y_pred)
    conf_matx = confusion_matrix(test_Y, y_pred)
    f1score = f1_score(test_Y, y_pred, average="macro")
    precision = precision_score(test_Y, y_pred, average="macro")
    recall = recall_score(test_Y, y_pred, average="macro")

    print(f"F-Score: {f1score}")
    print(f"Precision: {precision}")
    print(f"Re-call: {recall}")
    print(f"Accuracy: {accuracy}")
    print(f"Confusion Matrix:\n{conf_matx}")

    clrp = classification_report(test_Y, y_pred, target_names=label_names)
    print(clrp)

    class_far = calculate_false_alarm_rate(conf_matx, label_names)
    for label_name, false_alarm in class_far.items():
        print(f"False Alarm of {label_name}: {false_alarm:.4f} ({false_alarm * 100:.2f}%)")

    overall_far = sum(class_far.values()) / len(class_far)
    print(f"Overall False Alarm Rate: {overall_far:.4f} ({overall_far * 100:.2f}%)")

    print(f"Training Time: {train_time:.4f} seconds")  # Print training time
    print(f"Test Time: {test_time:.4f} seconds")  # Print test time
    total_time = train_time + test_time
    print(f"Total Time: {total_time:.4f} seconds")

    # Calculate the error rate
    error_rate = 1 - accuracy
    print(f"Error Rate: {error_rate:.4f} ({error_rate * 100:.2f}%)")

    # Create the heatmap using Matplotlib
    plt.imshow(conf_matx, interpolation='nearest', cmap='plasma')
    plt.title("Confusion Matrix")
    plt.colorbar()

    # Label the axes with numbers
    for i in range(len(label_names)):
        for j in range(len(label_names)):
            plt.text(j, i, str(conf_matx[i, j]), ha='center', va='center', color='white')

    # Label the axes
    plt.xlabel("Predicted")
    plt.ylabel("Actual")
    plt.xticks(np.arange(len(label_names)), label_names, rotation=45)
    plt.yticks(np.arange(len(label_names)), label_names)

    # Append performance metrics to the list
    performance_metrics.append({
        'F-Score': f1score,
        'Precision': precision,
        'Recall': recall,
        'Accuracy': accuracy,
        'Overall FAR': overall_far,
        'Training Time': train_time,
        'Test Time': test_time,
        'Total Time': total_time,
        'Error Rate': error_rate
    })

    plt.show()

def calculate_false_alarm_rate(confusion_matrix, label_names):
    class_far = {}
    for i in range(len(label_names)):
        false_alarms = sum(confusion_matrix[j][i] for j in range(len(label_names)) if j != i)
        class_far[label_names[i]] = false_alarms / sum(confusion_matrix[i])
    return class_far

# Define label_names based on your dataset
label_names = ['normal', 'dos', 'probe', 'r2l', 'u2r']

classifiers = [
    DecisionTreeClassifier,
    RandomForestClassifier,
    SVC,
    KNeighborsClassifier,
    LogisticRegression,
]

# Then, pass label_names as an argument when calling the analysis function
for classifier in classifiers:
    evaluate_classifier(classifier, train_x, train_Y, test_x, test_Y, label_names=label_names)

metrics_names = ['F-Score', 'Precision', 'Recall', 'Accuracy', 'FAR', 'Training Time', 'Test Time', 'Error Rate']
metrics_values = [list(metric.values()) for metric in performance_metrics]

plt.figure(figsize=(12, 12))  # Increase the figure size to accommodate additional charts

for i, metric_name in enumerate(metrics_names):
    plt.subplot(4, 2, i + 1)

    # Special handling for Time and Error Rate metrics
    if metric_name in ['Training Time', 'Test Time', 'Error Rate']:
        plt.bar([classifier.__name__ for classifier in classifiers], [values[i] for values in metrics_values])
    else:
        plt.plot([classifier.__name__ for classifier in classifiers], [values[i] for values in metrics_values], marker='o')

    plt.title(metric_name)
    plt.xlabel('Classifier')
    plt.xticks(rotation=45)
    plt.ylabel(metric_name)
    plt.grid(True)

plt.tight_layout()
plt.show()