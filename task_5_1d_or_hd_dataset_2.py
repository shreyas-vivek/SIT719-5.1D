# -*- coding: utf-8 -*-
"""Task 5.1D or HD - Dataset 2

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PnPXkbWvZTZe3-Bnnl9-THNdkCsdVPCh

# **Importing libraries**
"""

# Commented out IPython magic to ensure Python compatibility.
import os
from collections import defaultdict
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, precision_score, recall_score, classification_report
from sklearn.metrics import ConfusionMatrixDisplay
from sklearn.preprocessing import StandardScaler
import warnings
import seaborn as sns
from time import time

# %matplotlib inline
warnings.filterwarnings('ignore')

import pandas as pd

import io

dataset = pd.read_csv('Processed_Combined_IoT_dataset.csv')

dataset.head()

print(dataset.shape)

print(list(dataset.columns))

target_cols=list(dataset.columns[-1:])
target_cols

feature_cols= list(dataset.columns[:-1])
feature_cols

#split dataset in features and target variable
X = dataset.drop('label', axis=1) # Features
y = dataset['label'] # Target variable

X.head()

y.head()

# Split dataset into training set and test set
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1) # 70% training and 30% test

# Check the shape of all of these
print("X_train shape is : ", X_train.shape)
print("X_test shape  is : ", X_test.shape)
print("y_train shape is : ", y_train.shape)
print("y_test shape is  : ", y_test.shape)

# Define a list of classifier classes
classifiers = [
    DecisionTreeClassifier,
    RandomForestClassifier,
    KNeighborsClassifier,
    LogisticRegression,
    GaussianNB,
]

def evaluate_classifier(class_type, train_x, train_Y, test_x, test_Y, label_names=None, **kwargs):
    start_time = time()  # Record the start time for training

    if class_type == 'DecisionTree':
        from sklearn.tree import DecisionTreeClassifier
        classifier = DecisionTreeClassifier(**kwargs)
    elif class_type == 'RandomForest':
        from sklearn.ensemble import RandomForestClassifier
        classifier = RandomForestClassifier(**kwargs)
    elif class_type == 'KNeighbors':
        from sklearn.neighbors import KNeighborsClassifier
        classifier = KNeighborsClassifier(**kwargs)
    elif class_type == 'LogisticRegression':
        from sklearn.linear_model import LogisticRegression
        classifier = LogisticRegression(**kwargs)
    elif class_type == 'NaiveBayes':
        from sklearn.naive_bayes import GaussianNB
        classifier = GaussianNB(**kwargs)
    else:
        raise ValueError(f"Unsupported classifier type: {class_type}")

    classifier.fit(train_x, train_Y)
    training_time = time() - start_time  # Calculate training time

    start_time = time()  # Record the start time for testing
    y_pred = classifier.predict(test_x)
    testing_time = time() - start_time  # Calculate testing time

    class_name = class_type
    print(f"Classifier: {class_name}")
    print(f"Evaluation Report for {class_name}:")
    show_evaluation_results(test_Y, y_pred, label_names, training_time, testing_time)
    print("=" * 40)

label_names = list(map(str, np.unique(y_test)))
performance_metrics = []

def show_evaluation_results(test_Y, y_pred, label_names, training_time, testing_time):
    accuracy = accuracy_score(test_Y, y_pred)
    conf_matx = confusion_matrix(test_Y, y_pred)
    f1score = f1_score(test_Y, y_pred, average="macro")
    precision = precision_score(test_Y, y_pred, average="macro")
    recall = recall_score(test_Y, y_pred, average="macro")

    print(f"F-Score: {f1score}")
    print(f"Precision: {precision}")
    print(f"Re-call: {recall}")
    print(f"Accuracy: {accuracy}")
    print(f"Confusion Matrix:\n{conf_matx}")

    clrp = classification_report(test_Y, y_pred, target_names=label_names)
    print(clrp)

    class_far = calculate_false_alarm_rate(conf_matx, label_names)
    for label_name, false_alarm in class_far.items():
        print(f"False Alarm of {label_name}: {false_alarm:.4f} ({false_alarm * 100:.2f}%)")

    overall_far = sum(class_far.values()) / len(class_far)
    print(f"Overall False Alarm Rate: {overall_far:.4f} ({overall_far * 100:.2f}%)")

    # Add training time, testing time, and error rate to the report
    print(f"Training Time: {training_time:.2f} seconds")
    print(f"Testing Time: {testing_time:.2f} seconds")
    error_rate = 1 - accuracy
    print(f"Error Rate: {error_rate:.4f} ({error_rate * 100:.2f}%)")

    # Create the heatmap using Matplotlib
    plt.imshow(conf_matx, interpolation='nearest', cmap='plasma')
    plt.title("Confusion Matrix")
    plt.colorbar()

    # Label the axes with numbers
    for i in range(len(label_names)):
        for j in range(len(label_names)):
            plt.text(j, i, str(conf_matx[i, j]), ha='center', va='center', color='white')

    # Label the axes
    plt.xlabel("Predicted")
    plt.ylabel("Actual")
    plt.xticks(np.arange(len(label_names)), label_names, rotation=45)
    plt.yticks(np.arange(len(label_names)), label_names)

    # Append performance metrics to the list
    performance_metrics.append({
        'F-Score': f1score,
        'Precision': precision,
        'Recall': recall,
        'Accuracy': accuracy,
        'Overall FAR': overall_far,
        'Training Time': training_time,
        'Testing Time': testing_time,
        'Error Rate': error_rate
    })

    plt.show()

def calculate_false_alarm_rate(conf_matx, label_names):
    class_far = {}
    for i, label_name in enumerate(label_names):
        TP = conf_matx[i, i]
        FP = np.sum(conf_matx[:, i]) - TP
        FN = np.sum(conf_matx[i, :]) - TP
        TN = np.sum(conf_matx) - TP - FP - FN
        false_alarm = FP / (FP + TN)
        class_far[label_name] = false_alarm
    return class_far

classifiers = [
    ('DecisionTree', DecisionTreeClassifier(random_state=17)),
    ('RandomForest', RandomForestClassifier(random_state=17)),
    ('KNeighbors', KNeighborsClassifier()),
    ('LogisticRegression', LogisticRegression()),
    ('NaiveBayes', GaussianNB()),
]

for classifier_name, _ in classifiers:
    evaluate_classifier(class_type=classifier_name, train_x=X_train, train_Y=y_train, test_x=X_test, test_Y=y_test, label_names=label_names)

metrics_names = ['F-Score', 'Precision', 'Recall', 'Accuracy', 'Overall FAR', 'Training Time', 'Testing Time', 'Error Rate']
metrics_values = [list(metric.values()) for metric in performance_metrics]

plt.figure(figsize=(12, 12))  # Increase the figure size to accommodate additional charts

for i, metric_name in enumerate(metrics_names):
    plt.subplot(4, 2, i + 1)

    # Special handling for Time and Error Rate metrics
    if metric_name in ['Training Time', 'Testing Time', 'Error Rate']:
        plt.bar([classifier[0] for classifier in classifiers], [values[i] for values in metrics_values])
    else:
        plt.plot([classifier[0] for classifier in classifiers], [values[i] for values in metrics_values], marker='o')

    plt.title(metric_name)
    plt.xlabel('Classifier')
    plt.xticks(rotation=45)
    plt.ylabel(metric_name)
    plt.grid(True)

plt.tight_layout()
plt.show()